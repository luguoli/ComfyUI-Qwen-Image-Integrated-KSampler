import torch
import comfy.samplers
import comfy.model_management
from .cache import remove_cache
import numpy as np
from PIL import Image
import os
from .utils import *


class QwenImageIntegratedKSampler:

    def __init__(self):
        self.device = comfy.model_management.intermediate_device()

    @classmethod
    def INPUT_TYPES(s):
        generation_mode = ['æ–‡ç”Ÿå›¾ text-to-image', 'å›¾ç”Ÿå›¾ image-to-image']
        return {
            "required": {
                "model": ("MODEL", {}),
                "clip": ("CLIP", ),
                "vae": ("VAE", {}),
                "positive_prompt": ("STRING", {"multiline": True, "dynamicPrompts": True, "placeholder": "æ­£å‘æç¤ºè¯ positive_prompt"}),
                "negative_prompt": ("STRING", {"multiline": True, "dynamicPrompts": True, "placeholder": "è´Ÿå‘æç¤ºè¯ negative_prompt"}),
                "generation_mode": (generation_mode,),
                "batch_size": ("INT", {"default": 1, "min": 1, "max": 10}),
                "width": ("INT", {"default": 0, "min": 0, "max": 16384, "step": 8}),
                "height": ("INT", {"default": 0, "min": 0, "max": 16384, "step": 8}),
                "seed": ("INT", {"default": 0, "min": 0, "max": 0xffffffffffffffff, "control_after_generate": True}),
                "steps": ("INT", {"default": 4, "min": 1, "max": 10000}),
                "cfg": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 100.0, "step":0.1, "round": 0.01}),
                "sampler_name": (comfy.samplers.KSampler.SAMPLERS, {"default": "euler"}),
                "scheduler": (comfy.samplers.KSampler.SCHEDULERS, {"default": "simple"}),
                "denoise": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 1.0, "step": 0.01}),
            },
            "optional": {
                "image1": ("IMAGE", ),
                "image2": ("IMAGE", ),
                "image3": ("IMAGE", ),
                "image4": ("IMAGE", ),
                "image5": ("IMAGE", ),
                "latent": ("LATENT", {}),
                "auraflow_shift": ("FLOAT", {"default": 3.0, "min": 0.0, "max": 100.0, "step": 0.01}),
                "cfg_norm_strength": ("FLOAT", {"default": 1, "min": 0.0, "max": 100.0, "step": 0.01}),
                "enable_clean_gpu_memory": ("BOOLEAN", {"default": False}),
                "enable_clean_cpu_memory_after_finish": ("BOOLEAN", {"default": False}),
                "enable_sound_notification": ("BOOLEAN", {"default": False}),
                "auto_save_output_folder": ("STRING", {"default": ""}),
                "output_filename_prefix": ("STRING", {"default": "auto_save"}),
                "instruction": ("STRING", {"multiline": True, "default": "Describe the key features of the input image (color, shape, size, texture, objects, background), then explain how the user's text instruction should alter or modify the image. Generate a new image that meets the user's requirements while maintaining consistency with the original input where appropriate.", "placeholder": "ä¸å»ºè®®ä¿®æ”¹ Not recommended to modify"}),
            }
        }

    RETURN_TYPES = ("IMAGE", "LATENT", "IMAGE")
    RETURN_NAMES = ("ç”Ÿæˆå›¾åƒImage", "Latent", "ç¼©æ”¾ååŸå›¾Scaled Image")
    FUNCTION = "sample"
    CATEGORY = "sampling"
    # æ³¨æ„è¯­è¨€æ–‡ä»¶ä¸­ä¸èƒ½ç”¨@ç¬¦å·
    DESCRIPTION = "ğŸ‹ åƒé—®å›¾åƒé›†æˆé‡‡æ ·å™¨ - Ké‡‡æ ·å™¨ï¼Œæ™ºèƒ½å¤šæ¨¡æ€é‡‡æ ·å™¨ï¼Œæ”¯æŒæ–‡ç”Ÿå›¾/å›¾ç”Ÿå›¾åŒæ¨¡å¼ï¼Œä¼˜åŒ–å®˜æ–¹åç§»é—®é¢˜ï¼Œæ›´éµä»æŒ‡ä»¤ï¼Œå›¾ç‰‡ç¼©æ”¾ã€å¯å¤„ç†å¤šå¼ å‚è€ƒå›¾ã€è‡ªåŠ¨æ˜¾å­˜/å†…å­˜ç®¡ç†ã€æ‰¹é‡ç”Ÿæˆã€è‡ªåŠ¨ä¿å­˜ã€å£°éŸ³é€šçŸ¥ã€AuraFlowä¼˜åŒ–ã€CFGæ ‡å‡†åŒ–è°ƒèŠ‚ç­‰å…¨æ–¹ä½åŠŸèƒ½ï¼Œä¸éœ€è¦è¿é‚£ä¹ˆå¤šçº¿å•¦~~~~/ğŸ‹ Qwen Image Integrated KSampler - KSampler, intelligent multimodal sampler, supports text-to-image / image-to-image dual modes, optimizes official offset issues, better complies with instructions, image scaling, can process multiple reference images, automatic VRAM/RAM management, batch generation, automatic saving, sound notification, AuraFlow optimization, CFG normalization adjustment and other comprehensive functions, no need to connect so many wires~~~~"


    


    def sample(self, model, clip, vae, positive_prompt, negative_prompt, generation_mode, batch_size, width, height, seed, steps, cfg, sampler_name, scheduler, denoise=1.0, image1=None, image2=None, image3=None, image4=None, image5=None, latent=None, auraflow_shift=0, cfg_norm_strength=0, enable_clean_gpu_memory=False, enable_clean_cpu_memory_after_finish=False, enable_sound_notification=False, instruction="", auto_save_output_folder="", output_filename_prefix="auto_save"):


        # Print start execution information
        print(f"ğŸ¯ å¼€å§‹æ‰§è¡Œé‡‡æ ·ä»»åŠ¡/Starting sampling task ......")
        print(f"ğŸ² ç§å­/Seed: {seed}")
        print(f"ğŸ“Š æ­¥éª¤æ•°/Steps: {steps}")
        print(f"ğŸ›ï¸ CFGå¼ºåº¦/CFG Scale: {cfg}")
        print(f"ğŸ”„ é™å™ªå¼ºåº¦/Denoise: {denoise}")
        print(f"ğŸŒ€ é‡‡æ ·å™¨/Sampler: {sampler_name}")
        print(f"ğŸ“ˆ è°ƒåº¦å™¨/Scheduler: {scheduler}")
        print(f"ğŸ“ åˆ†è¾¨ç‡/Resolution: {width} x {height}")
        print(f"ğŸ¯ ç”Ÿæˆæ¨¡å¼/Generation Mode: {generation_mode}")
        print(f"ğŸ”¢ æ‰¹é‡å¤§å°/Batch Size: {batch_size}")
        print(f"âœ¨ AuraFlow Shift: {auraflow_shift}")
        print(f"ğŸ›ï¸ CFGè§„èŒƒåŒ–å¼ºåº¦/CFG Normalization Strength: {cfg_norm_strength}")
        print(f"ğŸ§  æ­£å‘æç¤ºè¯é•¿åº¦/Positive Prompt Length: {len(positive_prompt)}")
        print(f"ğŸš« è´Ÿå‘æç¤ºè¯é•¿åº¦/Negative Prompt Length: {len(negative_prompt)}")
        print(f"ğŸ“ æŒ‡ä»¤é•¿åº¦/Instruction Length: {len(instruction)}")
        print(f"ğŸ—‚ï¸ è‡ªåŠ¨ä¿å­˜æ–‡ä»¶å¤¹/Auto Save Folder: {auto_save_output_folder if auto_save_output_folder else 'Disabled'}")
        print(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶åå‰ç¼€/Output Filename Prefix: {output_filename_prefix}")
        print(f"ğŸ§¹ æ¸…ç†GPUå†…å­˜/Clean GPU Memory: {enable_clean_gpu_memory}")
        print(f"ğŸ—‘ï¸ ç»“æŸåæ¸…ç†CPUå†…å­˜/Clean CPU Memory After Finish: {enable_clean_cpu_memory_after_finish}")
        print(f"ğŸ”Š å£°éŸ³é€šçŸ¥/Sound Notification: {enable_sound_notification}")

        # Initialize scaled images
        image1_scaled = image1


        if auraflow_shift > 0:
            print(f"âœ¨ åº”ç”¨shiftå‚æ•°/Applying shift parameter: {auraflow_shift}")
            m = model.clone()
            import comfy.model_sampling
            sampling_base = comfy.model_sampling.ModelSamplingDiscreteFlow
            sampling_type = comfy.model_sampling.CONST
            class ModelSamplingAdvanced(sampling_base, sampling_type):
                pass
            model_sampling = ModelSamplingAdvanced(m.model.model_config)
            model_sampling.set_parameters(shift=auraflow_shift, multiplier=1.0)
            m.add_object_patch("model_sampling", model_sampling)
            model = m
            print("âœ… shiftå‚æ•°å·²åº”ç”¨æˆåŠŸ/Shift parameter applied successfully")

        if cfg_norm_strength > 0:
            print(f"ğŸ›ï¸ åº”ç”¨å¼ºåº¦/Applying strength: {cfg_norm_strength}")
            m = model.clone()
            def cfg_norm(args):
                cond_p = args['cond_denoised']
                pred_text_ = args["denoised"]
                norm_full_cond = torch.norm(cond_p, dim=1, keepdim=True)
                norm_pred_text = torch.norm(pred_text_, dim=1, keepdim=True)
                scale = (norm_full_cond / (norm_pred_text + 1e-8)).clamp(min=0.0, max=1.0)
                return pred_text_ * scale * cfg_norm_strength
            m.set_model_sampler_post_cfg_function(cfg_norm)
            model = m
            print("âœ… è§„èŒƒåŒ–å·²åº”ç”¨æˆåŠŸ/Normalization applied successfully")




        

        if generation_mode == "å›¾ç”Ÿå›¾ image-to-image":

            if image1 is None:
                raise Exception("å›¾ç”Ÿå›¾å¿…é¡»è‡³å°‘è¾“å…¥ä¸€å¼ å›¾ç‰‡ï¼Œè¯·è¾“å…¥å›¾åƒ1ï¼ˆä¸»å›¾ï¼‰ã€‚You must enter at least one image. Please enter image 1 (main image).")

            # Scale reference images if needed

            images_scaled = [image1, image2, image3, image4, image5]

            if width > 0 and height > 0:
                print(f"ğŸ“ [Image Scale] å°†å›¾åƒç¼©æ”¾è‡³ / Scaling reference images to {width}x{height}")

                for i, img in enumerate(images_scaled):
                    if img is not None:
                        # try:
                            scaled_image, _, _, _, _ = image_scale_by_aspect_ratio('original', 1, 1, 'letterbox', 'lanczos', '8', 'max_size', (width, height), '#000000', img, None)
                            images_scaled[i] = scaled_image
                        # except Exception as e:
                        #     log(f"âš ï¸ [Image Scale] Cannot scale image {i+1} with shape {img.shape}: {e}")
                        #     images_scaled[i] = img
                    # else: None

                print("âœ… [Image Scale] å›¾åƒç¼©æ”¾å®Œæˆ / Reference images scaled successfully")

            image1_scaled, image2_scaled, image3_scaled, image4_scaled, image5_scaled = images_scaled

            image_prompt, images_vl, llama_template, ref_latents = get_image_prompt(vae, image1_scaled, image2_scaled, image3_scaled, image4_scaled, image5_scaled, upscale_method="lanczos", crop="disabled", instruction=instruction)

            positive = prompt_encode(clip, positive_prompt, image_prompt=image_prompt, images_vl=images_vl, llama_template=llama_template, ref_latents=ref_latents)
            negative = prompt_encode(clip, negative_prompt, image_prompt=image_prompt, images_vl=images_vl, llama_template=llama_template, ref_latents=ref_latents)
        else:
            if width > 0 and height > 0:
                positive = prompt_encode(clip, positive_prompt)
                negative = prompt_encode(clip, negative_prompt)
            else:
                raise Exception("æ–‡ç”Ÿå›¾å¿…é¡»è¾“å…¥å®½é«˜ã€‚text-to-image width and height must be entered.")

        if latent is None:
            if image1_scaled is not None:
                samples = vae.encode(image1_scaled[:,:,:,:3])
                if batch_size > 1:
                    samples = samples.repeat((batch_size,) + ((1,) * (samples.ndim - 1)))
            else:
                samples = torch.zeros([batch_size, 4, height // 8, width // 8], device=self.device)
            latent = {"samples":samples}

        

        


        print("ğŸš€ å¼€å§‹é‡‡æ ·è¿‡ç¨‹/Starting Sampling...")
        print(f"ğŸ“Š æ€»æ­¥æ•°/Total Steps: {steps}")

        if enable_clean_gpu_memory:
            print("ğŸ—‘ï¸ é¢„æ¸…ç†æ˜¾å­˜å ç”¨/Pre-cleaning GPU memory...")
            try:
                cleanGPUUsedForce()
                remove_cache('*')
            except ImportError:
                print("ğŸ”• æ˜¾å­˜æ¸…ç†å¤±è´¥/Pre GPU memory cleaning failed")
            print("âœ… é¢„æ˜¾å­˜æ¸…ç†å®Œæˆ/Pre GPU memory cleaning completed")

        latent_output = common_ksampler(model, seed, steps, cfg, sampler_name, scheduler, positive, negative, latent, denoise=denoise)

        print("ğŸ–¼ï¸ æ­£åœ¨è§£ç æ½œç©ºé—´/Decoding latent space...")
        output_images = vae.decode(latent_output["samples"])
        if len(output_images.shape) == 5: #Combine batches
            output_images = output_images.reshape(-1, output_images.shape[-3], output_images.shape[-2], output_images.shape[-1])
        print("âœ… è§£ç å®Œæˆ/Decoding completed")



        if auto_save_output_folder:
            try:
                import folder_paths
                output_filename_prefix = output_filename_prefix or "auto_save"
                if os.path.isabs(auto_save_output_folder):
                    full_output_folder = auto_save_output_folder
                else:
                    output_dir = folder_paths.get_output_directory()
                    full_output_folder = os.path.join(output_dir, auto_save_output_folder)

                if not os.path.exists(full_output_folder):
                    os.makedirs(full_output_folder, exist_ok=True)

                print(f"ğŸ’¾ [Auto Save] è‡ªåŠ¨ä¿å­˜æ–‡ä»¶è‡³ / Saving images to ã€{full_output_folder}ã€‘")

                for batch_number, image in enumerate(output_images):
                    img = Image.fromarray((image.cpu().numpy() * 255).astype(np.uint8))
                    file = f"{output_filename_prefix}_{seed}_{batch_number:05}.png"
                    img.save(os.path.join(full_output_folder, file))

                print("âœ… [Auto Save] è‡ªåŠ¨ä¿å­˜æ–‡ä»¶æˆåŠŸ / Images saved successfully")
            except ImportError:
                print("ğŸ”• è‡ªåŠ¨ä¿å­˜æ–‡ä»¶å¤±è´¥/ Images saved failed")


        if enable_clean_gpu_memory:
            print("ğŸ—‘ï¸ åæ¸…ç†æ˜¾å­˜å ç”¨/Post-cleaning GPU memory...")
            try:
                cleanGPUUsedForce()
                remove_cache('*')
            except ImportError:
                print("ğŸ”• æ˜¾å­˜æ¸…ç†å¤±è´¥/Pre GPU memory cleaning failed")
            print("âœ… åæ˜¾å­˜æ¸…ç†å®Œæˆ/Post GPU memory cleaning completed")

        if enable_clean_cpu_memory_after_finish:
            print("ğŸ—‘ï¸ å®Œæˆåæ¸…ç†CPUå†…å­˜/Post-cleaning CPU memory after finish...")
            try:
                clean_ram(clean_file_cache=True, clean_processes=True, clean_dlls=True, retry_times=3)
            except Exception as e:
                print(f"ğŸ”• RAMæ¸…ç†å¤±è´¥/RAM cleanup failed: {str(e)}")
            else:
                print("âœ… [Clean CPU Memory After Finish] RAMæ¸…ç†å®Œæˆ / RAM cleanup completed")

        if enable_sound_notification:
            try:
                import winsound
                import time
                # æ’­æ”¾å¿«é€Ÿç´§å‡‘çš„æ—‹å¾‹ï¼šA4, C5, E5, G5ï¼Œè¾ƒçŸ­é—´éš”ä½¿æ—‹å¾‹è¿è´¯
                frequencies = [440, 523, 659, 784]
                for freq in frequencies:
                    winsound.Beep(freq, 150)
                    time.sleep(0.005)  # æ›´çŸ­é—´éš”åŠ å¿«èŠ‚å¥
                print("ğŸµ [Sound Notification] Completion melody played")
            except ImportError:
                print("ğŸ”• [Sound Notification] Sound notification not supported on this system")
            except Exception as e:
                print(f"ğŸ”• [Sound Notification] Audio playback failed: {str(e)}")

        return (output_images, latent_output, image1_scaled)
        


    def set_shift(self, model, sigma_shift):
        """è®¾ç½®AuraFlowæ¨¡å‹çš„shiftå‚æ•°"""
        import comfy.model_sampling

        model_sampling = model.get_model_object("model_sampling")
        if not model_sampling:
            sampling_base = comfy.model_sampling.ModelSamplingDiscreteFlow
            sampling_type = comfy.model_sampling.CONST
            class ModelSamplingAdvanced(sampling_base, sampling_type):
                pass
            model_sampling = ModelSamplingAdvanced(model.model.model_config)

        model_sampling.set_parameters(shift=sigma_shift / 1000.0 * 100, multiplier=1000)
        model.add_object_patch("model_sampling", model_sampling)
        return model

class ExtraOptions:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {

            }
        }

    RETURN_TYPES = ("EXTRA_OPTIONS",)
    RETURN_NAMES = ("é¢å¤–è®¾å®š/Extra Options",)
    FUNCTION = "get_options"
    CATEGORY = "sampling"
    DESCRIPTION = "ğŸ›ï¸ é¢å¤–è®¾å®š/Extra Options - é«˜çº§å‚æ•°è®¾ç½®"

    def get_options(self):
        options = {

        }
        return (options,)

NODE_CLASS_MAPPINGS = {
    "QwenImageIntegratedKSampler": QwenImageIntegratedKSampler,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "QwenImageIntegratedKSampler": "ğŸ‹ åƒé—®å›¾åƒé›†æˆé‡‡æ ·å™¨â€”â€”Github:@luguoli",
}
