import torch
import comfy.samplers
import comfy.model_management
import comfy.controlnet
import comfy.cldm.control_types
from comfy.cldm.control_types import UNION_CONTROLNET_TYPES
import folder_paths
from .cache import remove_cache
import numpy as np
from PIL import Image
import os
from .utils import *


class QwenImageIntegratedKSampler:

    def __init__(self):
        self.device = comfy.model_management.intermediate_device()

    @classmethod
    def INPUT_TYPES(s):
        generation_mode = ['æ–‡ç”Ÿå›¾ text-to-image', 'å›¾ç”Ÿå›¾ image-to-image']
        return {
            "required": {
                "model": ("MODEL", {}),
                "clip": ("CLIP", ),
                "vae": ("VAE", {}),
                "positive_prompt": ("STRING", {"multiline": True, "dynamicPrompts": True, "placeholder": "æ­£å‘æç¤ºè¯ positive_prompt"}),
                "negative_prompt": ("STRING", {"multiline": True, "dynamicPrompts": True, "placeholder": "è´Ÿå‘æç¤ºè¯ negative_prompt"}),
                "generation_mode": (generation_mode,),
                "batch_size": ("INT", {"default": 1, "min": 1, "max": 10}),
                "width": ("INT", {"default": 0, "min": 0, "max": 16384, "step": 8}),
                "height": ("INT", {"default": 0, "min": 0, "max": 16384, "step": 8}),
                "seed": ("INT", {"default": 0, "min": 0, "max": 0xffffffffffffffff, "control_after_generate": True}),
                "steps": ("INT", {"default": 4, "min": 1, "max": 10000}),
                "cfg": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 100.0, "step":0.1, "round": 0.01}),
                "sampler_name": (comfy.samplers.KSampler.SAMPLERS, {"default": "euler"}),
                "scheduler": (comfy.samplers.KSampler.SCHEDULERS, {"default": "simple"}),
                "denoise": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 1.0, "step": 0.01}),
            },
            "optional": {
                "image1": ("IMAGE", ),
                "image2": ("IMAGE", ),
                "image3": ("IMAGE", ),
                "image4": ("IMAGE", ),
                "image5": ("IMAGE", ),
                "latent": ("LATENT", {}),
                "controlnet_data": ("CONTROL_NET_DATA", ),
                "auraflow_shift": ("FLOAT", {"default": 3.0, "min": 0.0, "max": 100.0, "step": 0.01}),
                "cfg_norm_strength": ("FLOAT", {"default": 1, "min": 0.0, "max": 100.0, "step": 0.01}),
                "enable_clean_gpu_memory": ("BOOLEAN", {"default": False}),
                "enable_clean_cpu_memory_after_finish": ("BOOLEAN", {"default": False}),
                "enable_sound_notification": ("BOOLEAN", {"default": False}),
                "auto_save_output_folder": ("STRING", {"default": ""}),
                "output_filename_prefix": ("STRING", {"default": "auto_save"}),
                "instruction": ("STRING", {"multiline": True, "default": "Describe the key features of the input image (color, shape, size, texture, objects, background), then explain how the user's text instruction should alter or modify the image. Generate a new image that meets the user's requirements while maintaining consistency with the original input where appropriate.", "placeholder": "ä¸å»ºè®®ä¿®æ”¹ Not recommended to modify"}),
            }
        }

    RETURN_TYPES = ("IMAGE", "LATENT", "IMAGE")
    RETURN_NAMES = ("ç”Ÿæˆå›¾åƒImage", "ï¼ˆå¯é€‰ï¼‰Latent", "ç¼©æ”¾ååŸå›¾Scaled Image")
    FUNCTION = "sample"
    CATEGORY = "sampling"
    # æ³¨æ„è¯­è¨€æ–‡ä»¶ä¸­ä¸èƒ½ç”¨@ç¬¦å·
    DESCRIPTION = "ğŸ‹ åƒé—®å›¾åƒé›†æˆé‡‡æ ·å™¨ - Ké‡‡æ ·å™¨ï¼Œæ™ºèƒ½å¤šæ¨¡æ€é‡‡æ ·å™¨ï¼Œæ”¯æŒæ–‡ç”Ÿå›¾/å›¾ç”Ÿå›¾åŒæ¨¡å¼ï¼Œä¼˜åŒ–å®˜æ–¹åç§»é—®é¢˜ï¼Œæ›´éµä»æŒ‡ä»¤ï¼Œå›¾ç‰‡ç¼©æ”¾ã€å¯å¤„ç†å¤šå¼ å‚è€ƒå›¾ã€è‡ªåŠ¨æ˜¾å­˜/å†…å­˜ç®¡ç†ã€æ‰¹é‡ç”Ÿæˆã€è‡ªåŠ¨ä¿å­˜ã€å£°éŸ³é€šçŸ¥ã€AuraFlowä¼˜åŒ–ã€CFGæ ‡å‡†åŒ–è°ƒèŠ‚ç­‰å…¨æ–¹ä½åŠŸèƒ½ï¼Œä¸éœ€è¦è¿é‚£ä¹ˆå¤šçº¿å•¦~~~~/ğŸ‹ Qwen Image Integrated KSampler - KSampler, intelligent multimodal sampler, supports text-to-image / image-to-image dual modes, optimizes official offset issues, better complies with instructions, image scaling, can process multiple reference images, automatic VRAM/RAM management, batch generation, automatic saving, sound notification, AuraFlow optimization, CFG normalization adjustment and other comprehensive functions, no need to connect so many wires~~~~"


    


    def sample(self, model, clip, vae, positive_prompt, negative_prompt, generation_mode, batch_size, width, height, seed, steps, cfg, sampler_name, scheduler, denoise=1.0, image1=None, image2=None, image3=None, image4=None, image5=None, latent=None, controlnet_data=None, auraflow_shift=0, cfg_norm_strength=0, enable_clean_gpu_memory=False, enable_clean_cpu_memory_after_finish=False, enable_sound_notification=False, instruction="", auto_save_output_folder="", output_filename_prefix="auto_save"):


        # Print start execution information
        print(f"ğŸ¯ å¼€å§‹æ‰§è¡Œé‡‡æ ·ä»»åŠ¡/Starting sampling task ......")
        print(f"ğŸ² ç§å­/Seed: {seed}")
        print(f"ğŸ“Š æ­¥éª¤æ•°/Steps: {steps}")
        print(f"ğŸ›ï¸ CFGå¼ºåº¦/CFG Scale: {cfg}")
        print(f"ğŸ”„ é™å™ªå¼ºåº¦/Denoise: {denoise}")
        print(f"ğŸŒ€ é‡‡æ ·å™¨/Sampler: {sampler_name}")
        print(f"ğŸ“ˆ è°ƒåº¦å™¨/Scheduler: {scheduler}")
        print(f"ğŸ“ åˆ†è¾¨ç‡/Resolution: {width} x {height}")
        print(f"ğŸ¯ ç”Ÿæˆæ¨¡å¼/Generation Mode: {generation_mode}")
        print(f"ğŸ”¢ æ‰¹é‡å¤§å°/Batch Size: {batch_size}")
        print(f"âœ¨ AuraFlow Shift: {auraflow_shift}")
        print(f"ğŸ›ï¸ CFGè§„èŒƒåŒ–å¼ºåº¦/CFG Normalization Strength: {cfg_norm_strength}")
        print(f"ğŸ§  æ­£å‘æç¤ºè¯é•¿åº¦/Positive Prompt Length: {len(positive_prompt)}")
        print(f"ğŸš« è´Ÿå‘æç¤ºè¯é•¿åº¦/Negative Prompt Length: {len(negative_prompt)}")
        print(f"ğŸ“ æŒ‡ä»¤é•¿åº¦/Instruction Length: {len(instruction)}")
        print(f"ğŸ—‚ï¸ è‡ªåŠ¨ä¿å­˜æ–‡ä»¶å¤¹/Auto Save Folder: {auto_save_output_folder if auto_save_output_folder else 'Disabled'}")
        print(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶åå‰ç¼€/Output Filename Prefix: {output_filename_prefix}")
        print(f"ğŸ§¹ æ¸…ç†GPUå†…å­˜/Clean GPU Memory: {enable_clean_gpu_memory}")
        print(f"ğŸ—‘ï¸ ç»“æŸåæ¸…ç†CPUå†…å­˜/Clean CPU Memory After Finish: {enable_clean_cpu_memory_after_finish}")
        print(f"ğŸ”Š å£°éŸ³é€šçŸ¥/Sound Notification: {enable_sound_notification}")

        # Initialize scaled images
        image1_scaled = image1


        if auraflow_shift > 0:
            print(f"âœ¨ åº”ç”¨shiftå‚æ•°/Applying shift parameter: {auraflow_shift}")
            m = model.clone()
            import comfy.model_sampling
            sampling_base = comfy.model_sampling.ModelSamplingDiscreteFlow
            sampling_type = comfy.model_sampling.CONST
            class ModelSamplingAdvanced(sampling_base, sampling_type):
                pass
            model_sampling = ModelSamplingAdvanced(m.model.model_config)
            model_sampling.set_parameters(shift=auraflow_shift, multiplier=1.0)
            m.add_object_patch("model_sampling", model_sampling)
            model = m
            print("âœ… shiftå‚æ•°å·²åº”ç”¨æˆåŠŸ/Shift parameter applied successfully")

        if cfg_norm_strength > 0:
            print(f"ğŸ›ï¸ åº”ç”¨å¼ºåº¦/Applying strength: {cfg_norm_strength}")
            m = model.clone()
            def cfg_norm(args):
                cond_p = args['cond_denoised']
                pred_text_ = args["denoised"]
                norm_full_cond = torch.norm(cond_p, dim=1, keepdim=True)
                norm_pred_text = torch.norm(pred_text_, dim=1, keepdim=True)
                scale = (norm_full_cond / (norm_pred_text + 1e-8)).clamp(min=0.0, max=1.0)
                return pred_text_ * scale * cfg_norm_strength
            m.set_model_sampler_post_cfg_function(cfg_norm)
            model = m
            print("âœ… è§„èŒƒåŒ–å·²åº”ç”¨æˆåŠŸ/Normalization applied successfully")




        

        if generation_mode == "å›¾ç”Ÿå›¾ image-to-image":

            if image1 is None:
                raise Exception("å›¾ç”Ÿå›¾å¿…é¡»è‡³å°‘è¾“å…¥ä¸€å¼ å›¾ç‰‡ï¼Œè¯·è¾“å…¥å›¾åƒ1ï¼ˆä¸»å›¾ï¼‰ã€‚You must enter at least one image. Please enter image 1 (main image).")

            # Scale reference images if needed

            images_scaled = [image1, image2, image3, image4, image5]

            if width > 0 and height > 0:
                print(f"ğŸ“ [Image Scale] å°†å›¾åƒç¼©æ”¾è‡³ / Scaling reference images to {width}x{height}")

                for i, img in enumerate(images_scaled):
                    if img is not None:
                        # try:
                            scaled_image, _, _, _, _ = image_scale_by_aspect_ratio('original', 1, 1, 'letterbox', 'lanczos', '8', 'max_size', (width, height), '#000000', img, None)
                            images_scaled[i] = scaled_image
                        # except Exception as e:
                        #  log(f"âš ï¸ [Image Scale] Cannot scale image {i+1} with shape {img.shape}: {e}")
                        #     images_scaled[i] = img
                    # else: None

                print("âœ… [Image Scale] å›¾åƒç¼©æ”¾å®Œæˆ / Reference images scaled successfully")

            image1_scaled, image2_scaled, image3_scaled, image4_scaled, image5_scaled = images_scaled

            image_prompt, images_vl, llama_template, ref_latents = get_image_prompt(vae, image1_scaled, image2_scaled, image3_scaled, image4_scaled, image5_scaled, upscale_method="lanczos", crop="disabled", instruction=instruction)

            print("â³ [Prompt] æ­£åœ¨ç”Ÿæˆæ­£å‘æ¡ä»¶ / Generating positive condition...")
            positive = prompt_encode(clip, positive_prompt, image_prompt=image_prompt, images_vl=images_vl, llama_template=llama_template, ref_latents=ref_latents)
            print("â³ [Prompt] æ­£åœ¨ç”Ÿæˆè´Ÿå‘æ¡ä»¶ / Generating negative condition...")
            negative = prompt_encode(clip, negative_prompt, image_prompt=image_prompt, images_vl=images_vl, llama_template=llama_template, ref_latents=ref_latents)
            print("âœ… [Prompt] æç¤ºè¯æ¡ä»¶ç”Ÿæˆå®Œæˆ / Prompt generated successfully")


        else:
            if width > 0 and height > 0:
                positive = prompt_encode(clip, positive_prompt)
                negative = prompt_encode(clip, negative_prompt)
            else:
                raise Exception("æ–‡ç”Ÿå›¾å¿…é¡»è¾“å…¥å®½é«˜ã€‚text-to-image width and height must be entered.")

        # Apply ControlNet if provided
        if controlnet_data is not None:
            try:
                control_net = controlnet_data["control_net"]
                control_type = controlnet_data["control_type"]
                control_image = controlnet_data["image"]
                control_mask = controlnet_data["mask"]
                control_strength = controlnet_data["strength"]
                control_start_percent = controlnet_data["start_percent"]
                control_end_percent = controlnet_data["end_percent"]

                print(f"ğŸ¨ åº”ç”¨ControlNet/Applying ControlNet with strength: {control_strength}")

                if control_strength > 0:

                    if control_image is None:
                        raise Exception("ä½¿ç”¨ControlNetå¿…é¡»ä¼ å…¥æ§åˆ¶å›¾åƒã€‚ControlNet must enter control image.")
                    
                    extra_concat=[]

                    if control_type.lower() == "repaint" or control_type.lower() == "inpaint" or control_type.lower() == "inpainting" or control_type == "é‡ç»˜" or control_type == "å±€éƒ¨é‡ç»˜":
                        if control_mask is None:
                            raise Exception("ä½¿ç”¨å±€éƒ¨é‡ç»˜ControlNetå¿…é¡»ä¼ å…¥æ§åˆ¶é®ç½©ã€‚ControlNet repaint must enter control mask.")
                    
                    # å›¾ç”Ÿå›¾-å±€éƒ¨é‡ç»˜ æ—¶ä½¿ç”¨ç¼©æ”¾ä¸»å›¾
                    if generation_mode == "å›¾ç”Ÿå›¾ image-to-image":
                        if control_type.lower() == "repaint" or control_type.lower() == "inpaint" or control_type.lower() == "inpainting" or control_type == "é‡ç»˜" or control_type == "å±€éƒ¨é‡ç»˜":
                            
                            if width > 0 and height > 0:
                                scaled_control_image, scaled_control_mask, _, _, _ = image_scale_by_aspect_ratio('original', 1, 1, 'letterbox', 'lanczos', '8', 'max_size', (width, height), '#000000', control_image, control_mask)
                                control_image = scaled_control_image
                                control_mask = scaled_control_mask

                    if control_net.concat_mask and control_mask is not None:
                        control_mask = 1.0 - control_mask.reshape((-1, 1, control_mask.shape[-2], control_mask.shape[-1]))
                        control_mask_apply = comfy.utils.common_upscale(control_mask, control_image.shape[2], control_image.shape[1], "bilinear", "center").round()
                        control_image = control_image * control_mask_apply.movedim(1, -1).repeat(1, 1, 1, control_image.shape[3])
                        extra_concat = [control_mask]
                        print("âœ… ControlNet åº”ç”¨é®ç½©/ControlNet applied mask")

                    control_hint = control_image.movedim(-1,1)
                    cnets = {}

                    out = []
                    for conditioning in [positive, negative]:
                        c = []
                        for t in conditioning:
                            d = t[1].copy()

                            prev_cnet = d.get('control', None)
                            if prev_cnet in cnets:
                                c_net = cnets[prev_cnet]
                            else:
                                c_net = control_net.copy().set_cond_hint(control_hint, control_strength, (control_start_percent, control_end_percent), vae=vae, extra_concat=extra_concat)
                                c_net.set_previous_controlnet(prev_cnet)
                                cnets[prev_cnet] = c_net

                            d['control'] = c_net
                            d['control_apply_to_uncond'] = False
                            n = [t[0], d]
                            c.append(n)
                        out.append(c)

                    positive = out[0]
                    negative = out[1]
                    
                    print("âœ… ControlNet åº”ç”¨æˆåŠŸ/ControlNet applied successfully")
                else:
                    print("âš ï¸ ControlNetå¼ºåº¦è®¾ç½®ä¸º0ï¼Œä¸åº”ç”¨ControlNet/No ControlNet applied")
            except Exception as e:
                raise Exception(f"âš ï¸ [ControlNet] ControlNet åº”ç”¨å¤±è´¥ / Cannot apply ControlNet: {e}")



        if latent is None:
            if image1_scaled is not None:
                samples = vae.encode(image1_scaled[:,:,:,:3])
                if batch_size > 1:
                    samples = samples.repeat((batch_size,) + ((1,) * (samples.ndim - 1)))
            else:
                samples = torch.zeros([batch_size, 4, height // 8, width // 8], device=self.device)
            latent = {"samples":samples}

        

        


        print("ğŸš€ å¼€å§‹é‡‡æ ·è¿‡ç¨‹/Starting Sampling...")
        print(f"ğŸ“Š æ€»æ­¥æ•°/Total Steps: {steps}")

        if enable_clean_gpu_memory:
            print("ğŸ—‘ï¸ é¢„æ¸…ç†æ˜¾å­˜å ç”¨/Pre-cleaning GPU memory...")
            try:
                cleanGPUUsedForce()
                remove_cache('*')
            except ImportError:
                print("ğŸ”• æ˜¾å­˜æ¸…ç†å¤±è´¥/Pre GPU memory cleaning failed")
            print("âœ… é¢„æ˜¾å­˜æ¸…ç†å®Œæˆ/Pre GPU memory cleaning completed")

        latent_output = common_ksampler(model, seed, steps, cfg, sampler_name, scheduler, positive, negative, latent, denoise=denoise)

        print("ğŸ–¼ï¸ æ­£åœ¨è§£ç æ½œç©ºé—´/Decoding latent space...")
        output_images = vae.decode(latent_output["samples"])
        if len(output_images.shape) == 5: #Combine batches
            output_images = output_images.reshape(-1, output_images.shape[-3], output_images.shape[-2], output_images.shape[-1])
        print("âœ… è§£ç å®Œæˆ/Decoding completed")



        if auto_save_output_folder:
            try:
                import folder_paths
                output_filename_prefix = output_filename_prefix or "auto_save"
                if os.path.isabs(auto_save_output_folder):
                    full_output_folder = auto_save_output_folder
                else:
                    output_dir = folder_paths.get_output_directory()
                    full_output_folder = os.path.join(output_dir, auto_save_output_folder)

                if not os.path.exists(full_output_folder):
                    os.makedirs(full_output_folder, exist_ok=True)

                print(f"ğŸ’¾ [Auto Save] è‡ªåŠ¨ä¿å­˜æ–‡ä»¶è‡³ / Saving images to ã€{full_output_folder}ã€‘")

                for batch_number, image in enumerate(output_images):
                    img = Image.fromarray((image.cpu().numpy() * 255).astype(np.uint8))
                    file = f"{output_filename_prefix}_{seed}_{batch_number:05}.png"
                    img.save(os.path.join(full_output_folder, file))

                print("âœ… [Auto Save] è‡ªåŠ¨ä¿å­˜æ–‡ä»¶æˆåŠŸ / Images saved successfully")
            except ImportError:
                print("ğŸ”• è‡ªåŠ¨ä¿å­˜æ–‡ä»¶å¤±è´¥/ Images saved failed")


        if enable_clean_gpu_memory:
            print("ğŸ—‘ï¸ åæ¸…ç†æ˜¾å­˜å ç”¨/Post-cleaning GPU memory...")
            try:
                cleanGPUUsedForce()
                remove_cache('*')
            except ImportError:
                print("ğŸ”• æ˜¾å­˜æ¸…ç†å¤±è´¥/Pre GPU memory cleaning failed")
            print("âœ… åæ˜¾å­˜æ¸…ç†å®Œæˆ/Post GPU memory cleaning completed")

        if enable_clean_cpu_memory_after_finish:
            print("ğŸ—‘ï¸ å®Œæˆåæ¸…ç†CPUå†…å­˜/Post-cleaning CPU memory after finish...")
            try:
                clean_ram(clean_file_cache=True, clean_processes=True, clean_dlls=True, retry_times=3)
            except Exception as e:
                print(f"ğŸ”• RAMæ¸…ç†å¤±è´¥/RAM cleanup failed: {str(e)}")
            else:
                print("âœ… [Clean CPU Memory After Finish] RAMæ¸…ç†å®Œæˆ / RAM cleanup completed")

        if enable_sound_notification:
            try:
                import winsound
                import time
                # æ’­æ”¾å¿«é€Ÿç´§å‡‘çš„æ—‹å¾‹ï¼šA4, C5, E5, G5, E5, G5ï¼Œè¾ƒçŸ­é—´éš”ä½¿æ—‹å¾‹è¿è´¯
                frequencies = [440, 523, 659, 784, 659, 784]
                for freq in frequencies:
                    winsound.Beep(freq, 150)
                    time.sleep(0.005)  # æ›´çŸ­é—´éš”åŠ å¿«èŠ‚å¥
                print("ğŸµ [Sound Notification] Completion melody played")
            except ImportError:
                print("ğŸ”• [Sound Notification] Sound notification not supported on this system")
            except Exception as e:
                print(f"ğŸ”• [Sound Notification] Audio playback failed: {str(e)}")

        return (output_images, latent_output, image1_scaled)
        


    def set_shift(self, model, sigma_shift):
        """è®¾ç½®AuraFlowæ¨¡å‹çš„shiftå‚æ•°"""
        import comfy.model_sampling

        model_sampling = model.get_model_object("model_sampling")
        if not model_sampling:
            sampling_base = comfy.model_sampling.ModelSamplingDiscreteFlow
            sampling_type = comfy.model_sampling.CONST
            class ModelSamplingAdvanced(sampling_base, sampling_type):
                pass
            model_sampling = ModelSamplingAdvanced(model.model.model_config)

        model_sampling.set_parameters(shift=sigma_shift / 1000.0 * 100, multiplier=1000)
        model.add_object_patch("model_sampling", model_sampling)
        return model

class ExtraOptions:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {

            }
        }

    RETURN_TYPES = ("EXTRA_OPTIONS",)
    RETURN_NAMES = ("é¢å¤–è®¾å®š/Extra Options",)
    FUNCTION = "get_options"
    CATEGORY = "sampling"
    DESCRIPTION = "ğŸ›ï¸ é¢å¤–è®¾å®š/Extra Options - é«˜çº§å‚æ•°è®¾ç½®"

    def get_options(self):
        options = {

        }
        return (options,)

class QwenImageControlNetIntegratedLoader:
    @classmethod
    def INPUT_TYPES(s):
        type_options = ["auto"] + list(UNION_CONTROLNET_TYPES.keys())
        return {
            "required": {
                "image": ("IMAGE", ),
                "control_net_name": (folder_paths.get_filename_list("controlnet"), ),
                "control_type": (type_options, {"default": "auto"}),
                "strength": ("FLOAT", {"default": 2.0, "min": 0.0, "max": 10.0, "step": 0.01}),
                "start_percent": ("FLOAT", {"default": 0.0, "min": 0.0, "max": 1.0, "step": 0.001}),
                "end_percent": ("FLOAT", {"default": 0.5, "min": 0.0, "max": 1.0, "step": 0.001}),
            },
            "optional": {
                "mask": ("MASK", ),
            }
        }

    RETURN_TYPES = ("CONTROL_NET_DATA",)
    RETURN_NAMES = ("ControlNet æ•°æ®/ControlNet Data",)
    FUNCTION = "load_controlnet"
    CATEGORY = "conditioning/controlnet"
    DESCRIPTION = "ğŸ‹ åƒé—® ControlNet é›†æˆåŠ è½½å™¨ - éœ€è¦ ControlNet æ—¶ä½¿ç”¨/ğŸ‹ Qwen ControlNet Integrated Loader"

    def load_controlnet(self, image, control_net_name, control_type, strength, start_percent, end_percent, mask=None):

        # åŠ è½½ ControlNet
        controlnet = comfy.controlnet.load_controlnet(folder_paths.get_full_path_or_raise("controlnet", control_net_name))

        if controlnet is None:
            raise RuntimeError("ERROR: é”™è¯¯çš„æ§åˆ¶å™¨ / controlnet file is invalid and does not contain a valid controlnet model.")

        # è®¾ç½®æ§åˆ¶ç±»å‹
        controlnet = controlnet.copy()
        type_number = UNION_CONTROLNET_TYPES.get(control_type, -1)
        if type_number >= 0:
            controlnet.set_extra_arg("control_type", [type_number])
        else:
            controlnet.set_extra_arg("control_type", [])

        # åˆ›å»ºæ•°æ®å¯¹è±¡
        control_net_data = {
            "control_net": controlnet,
            "control_type": control_type,
            "image": image,
            "mask": mask,
            "strength": strength,
            "start_percent": start_percent,
            "end_percent": end_percent
        }

        return (control_net_data,)

NODE_CLASS_MAPPINGS = {
    "QwenImageIntegratedKSampler": QwenImageIntegratedKSampler,
    "QwenImageControlNetIntegratedLoader": QwenImageControlNetIntegratedLoader,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "QwenImageIntegratedKSampler": "ğŸ‹ åƒé—®å›¾åƒé›†æˆé‡‡æ ·å™¨â€”â€”Github:@luguoli",
    "QwenImageControlNetIntegratedLoader": "ğŸ‹ åƒé—®ControlNeté›†æˆåŠ è½½å™¨â€”â€”Github:@luguoli",
}
